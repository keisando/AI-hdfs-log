import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import gc
import re

# --- è¨­å®š ---
MODEL_PATH = 'models/lstm_model.h5'
CHUNK_SIZE = 50000   # 5ä¸‡è¡Œãšã¤é«˜é€Ÿå‡¦ç†
MICRO_BATCH = 256    # è¨ˆç®—ãƒãƒƒãƒã‚µã‚¤ã‚º
MAX_SEQ_LEN = 20

st.set_page_config(page_title="HDFS 10M Analysis", layout="wide")
st.title("ğŸ›¡ï¸ HDFS 10M Log Analysis (Final Production)")

# --- ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ ---
@st.cache_resource
def load_ai_model():
    with tf.device('/CPU:0'):
        return load_model(MODEL_PATH)

try:
    model = load_ai_model()
    st.sidebar.success("âœ… Model Loaded (16GB RAM)")
except Exception as e:
    st.error(f"Model Error: {e}")
    st.stop()

threshold = st.sidebar.slider("Anomaly Threshold", 0.0, 5.0, 0.35)

# --- ç¿»è¨³æ©Ÿèƒ½ï¼ˆã“ã“ãŒä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼ï¼‰ ---
def preprocess_events_robust(event_series):
    """
    "E5 E22" (æ–‡å­—) ãŒæ¥ã¦ã‚‚ [5, 22] (æ•°å­—) ã«å¼·åˆ¶å¤‰æ›ã™ã‚‹ã€‚
    ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ãŒå°‘ã—é•ã£ã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã‚’å‡ºã•ãªã„ã€‚
    """
    processed_seqs = []
    for item in event_series:
        tokens = []
        # æ–‡å­—åˆ—ã§ã‚‚ãƒªã‚¹ãƒˆã§ã‚‚å¯¾å¿œã§ãã‚‹ã‚ˆã†ã«çµ±ä¸€
        if isinstance(item, str):
            item_list = [item]
        elif isinstance(item, list) or isinstance(item, np.ndarray):
            item_list = item
        else:
            item_list = [str(item)]

        for sub_item in item_list:
            # ã‚«ãƒ³ãƒã‚„ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†è§£
            parts = str(sub_item).replace(',', ' ').split()
            for p in parts:
                # æ•°å­—ã ã‘æŠœãå‡ºã™ (E5 -> 5)
                clean_t = re.sub(r'[^0-9]', '', p)
                if clean_t.isdigit():
                    tokens.append(int(clean_t))
        
        # ä¸‡ãŒä¸€ç©ºã£ã½ãªã‚‰0ã‚’å…¥ã‚Œã‚‹
        if not tokens: tokens = [0]
        processed_seqs.append(tokens)
    return processed_seqs

# --- é«˜é€Ÿè¨ˆç®—é–¢æ•° ---
def calculate_anomaly_score_fast(model, X_data):
    scores = []
    dataset = tf.data.Dataset.from_tensor_slices(X_data).batch(MICRO_BATCH)
    scce = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)
    for batch in dataset:
        preds = model(batch, training=False)
        loss = scce(batch, preds)
        scores.extend(np.mean(loss.numpy(), axis=1))
        del preds, loss
    gc.collect()
    return np.array(scores)

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
st.info("ğŸ’¡ 1000ä¸‡è¡Œå¯¾å¿œæ¸ˆã¿ (Max 2GB Upload)")
uploaded_file = st.file_uploader("Upload CSV (BlockId, EventId)", type=['csv'])

if uploaded_file is not None:
    if st.button("ğŸš€ Start 10M Analysis"):
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        anomalies_only = []
        total_processed = 0
        
        # ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿
        chunk_iter = pd.read_csv(uploaded_file, chunksize=CHUNK_SIZE)
        
        for i, df_chunk in enumerate(chunk_iter):
            status_text.text(f"Processing... {total_processed:,} lines done.")
            
            try:
                # åˆ—ãƒã‚§ãƒƒã‚¯
                if 'EventId' not in df_chunk.columns:
                    st.error("CSVã« 'EventId' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ï¼å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ä½œã£ãŸCSVã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚")
                    st.stop()

                # â˜…ç¿»è¨³å®Ÿè¡Œ
                sequences = preprocess_events_robust(df_chunk['EventId'])
                
                # AIç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                X = pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')
                
                # æ¨è«–
                scores = calculate_anomaly_score_fast(model, X)
                
                df_chunk['Anomaly Score'] = scores
                df_chunk['Prediction'] = df_chunk['Anomaly Score'].apply(lambda x: 'Anomaly' if x > threshold else 'Normal')
                
                # ç•°å¸¸ã ã‘ä¿å­˜
                anomaly_df = df_chunk[df_chunk['Prediction'] == 'Anomaly']
                if not anomaly_df.empty:
                    anomalies_only.append(anomaly_df)
                
                total_processed += len(df_chunk)
                del X, sequences, scores
                gc.collect()

            except Exception as e:
                st.warning(f"Chunk {i} warning: {e}")
                continue

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
            progress = min((i + 1) / 200.0, 1.0)
            progress_bar.progress(progress)

        # --- å®Œäº†å¾Œ ---
        progress_bar.progress(1.0)
        status_text.success(f"âœ… Analysis Complete! Processed {total_processed:,} lines.")
        
        if anomalies_only:
            final_df = pd.concat(anomalies_only)
            # ä¸Šä½50ä»¶ã ã‘è¡¨ç¤º
            st.dataframe(final_df.sort_values('Anomaly Score', ascending=False).head(50))
            # å…¨ä»¶ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv = final_df.to_csv(index=False).encode('utf-8')
            st.download_button("Download Report", csv, "anomaly_report.csv", "text/csv")
        else:
            st.success("ğŸ‰ No anomalies found in this dataset.")